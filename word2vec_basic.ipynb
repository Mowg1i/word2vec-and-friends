{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec basic implementation\n",
    "\n",
    "See https://www.tensorflow.org/tutorials/word2vec\n",
    "And https://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n",
    "for further details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data into a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    \n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dictionary and replace rare words with UNK token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, vocabulary_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "\n",
    "        data.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate a training batch for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            \n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data in a usable form and print some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5241, 3083, 12, 6, 195, 2, 3136, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "filename = 'text8.zip'\n",
    "words = read_data(filename)\n",
    "print('Data size', len(words))\n",
    "vocabulary_size = 50000\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words, vocabulary_size)\n",
    "del words  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "batch_size = 128 # Size of each mini batch\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "num_steps = 100001 # Number of training steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph() # Explicitly create a graph\n",
    "\n",
    "with graph.as_default(): # Set it as default\n",
    "    # Input data\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        # Hidden layer\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        # Output layer\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  \n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(\n",
    "            valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Nearest to th: konya, ligases, sumner, heist, triple, scientist, holberg, narrow,\n",
      "Nearest to other: canine, grit, pinus, valerie, exotic, canal, hornby, dodi,\n",
      "Nearest to will: baluchistan, utrecht, divorces, roast, psych, jia, together, monogamous,\n",
      "Nearest to one: fixation, halloween, ag, hazards, recant, sequentially, rota, ambushed,\n",
      "Nearest to into: treatise, commodores, tux, hunnish, traditions, crowe, cuthbert, officiate,\n",
      "Nearest to which: conjectured, rapp, naaman, rdenas, xs, aho, selznick, suny,\n",
      "Nearest to of: massimo, puppies, sewing, sooner, maharaja, udma, bakkers, ballarat,\n",
      "Nearest to all: barracuda, novella, mooted, mannerisms, age, lamenting, gripping, tricks,\n",
      "Nearest to been: jumpers, animated, medians, una, adi, procession, aphrodite, mauritian,\n",
      "Nearest to five: polemic, tren, tipton, passion, tocharians, behavioral, zahir, entertainments,\n",
      "Nearest to however: ita, telekinetic, elamite, collapsing, breakup, mash, laser, government,\n",
      "Nearest to s: herbivores, reads, cult, sukarno, cliff, turmeric, drinks, airbases,\n",
      "Nearest to d: jordan, falling, hamel, nazism, cornerstone, vaux, argon, bahamas,\n",
      "Nearest to to: supple, offended, naked, memes, beer, vespasian, oakenfold, italy,\n",
      "Nearest to are: patristic, kidnapping, materialize, malory, adverts, capitalistic, previously, cytochrome,\n",
      "Nearest to these: arrogance, bjorn, norsemen, petitioner, alumnus, overtaken, transplanted, cultivating,\n",
      "Average loss at step  2000 :  113.756840047\n",
      "Average loss at step  4000 :  52.5790915697\n",
      "Average loss at step  6000 :  33.526589999\n",
      "Average loss at step  8000 :  23.3310797961\n",
      "Average loss at step  10000 :  17.8855012366\n",
      "Nearest to th: one, aberdeen, einstein, solf, scientist, eight, konya, reginae,\n",
      "Nearest to other: canal, coke, pinus, corrosion, akshara, canine, spinster, exotic,\n",
      "Nearest to will: bckgr, health, together, algeria, sod, manga, colony, utrecht,\n",
      "Nearest to one: two, zero, three, aberdeen, UNK, nine, coke, mosque,\n",
      "Nearest to into: traditions, hunnish, treatise, rand, is, around, co, letters,\n",
      "Nearest to which: UNK, selznick, aberdeen, savanna, rivalry, antimatter, victoriae, also,\n",
      "Nearest to of: in, and, for, victoriae, coke, reginae, or, with,\n",
      "Nearest to all: age, aberdeen, coke, novella, pleas, a, trails, boycott,\n",
      "Nearest to been: animated, and, victoriae, israel, are, retroviral, concern, finalist,\n",
      "Nearest to five: nine, zero, reginae, eight, three, one, passion, six,\n",
      "Nearest to however: government, ita, breakup, countable, hun, sky, exposing, cambridge,\n",
      "Nearest to s: and, rigid, reginae, the, of, canaris, zero, monastery,\n",
      "Nearest to d: falling, and, jordan, nazism, argon, orbitals, festival, reginae,\n",
      "Nearest to to: in, and, of, victoriae, with, nine, sites, quarks,\n",
      "Nearest to are: is, federation, were, and, was, colleagues, equal, retroviral,\n",
      "Nearest to these: alumnus, phi, bicycle, minister, imaging, consultative, turkey, cardinality,\n",
      "Average loss at step  12000 :  14.0381729831\n",
      "Average loss at step  14000 :  11.7468633239\n",
      "Average loss at step  16000 :  9.93615165854\n",
      "Average loss at step  18000 :  8.36480803037\n",
      "Average loss at step  20000 :  8.12387163663\n",
      "Nearest to th: one, eight, agouti, solf, einstein, six, circ, zero,\n",
      "Nearest to other: coke, canal, pinus, akshara, nunnery, amazons, canine, agouti,\n",
      "Nearest to will: but, agouti, utrecht, bckgr, together, to, algeria, sod,\n",
      "Nearest to one: two, three, agouti, circ, dasyprocta, six, four, eight,\n",
      "Nearest to into: from, within, perry, norse, hunnish, traditions, mugwort, treatise,\n",
      "Nearest to which: that, and, it, also, circ, UNK, two, selznick,\n",
      "Nearest to of: and, in, for, nine, eight, circ, agouti, victoriae,\n",
      "Nearest to all: age, aberdeen, novella, trails, coke, each, boycott, pleas,\n",
      "Nearest to been: animated, are, by, concern, mugwort, and, was, climbers,\n",
      "Nearest to five: nine, eight, zero, three, seven, two, four, six,\n",
      "Nearest to however: circ, ita, government, breakup, hun, exposing, folds, countable,\n",
      "Nearest to s: and, zero, the, his, circ, dasyprocta, of, agouti,\n",
      "Nearest to d: b, and, falling, cornerstone, nazism, one, lille, sulaiman,\n",
      "Nearest to to: in, and, circ, from, with, for, nine, zero,\n",
      "Nearest to are: were, is, was, agouti, have, federation, circ, been,\n",
      "Nearest to these: the, alumnus, they, this, bicycle, minister, which, consultative,\n",
      "Average loss at step  22000 :  6.96038261843\n",
      "Average loss at step  24000 :  6.81738079894\n",
      "Average loss at step  26000 :  6.73990534484\n",
      "Average loss at step  28000 :  6.48800724518\n",
      "Average loss at step  30000 :  5.93043940187\n",
      "Nearest to th: eight, agouti, six, one, solf, four, triple, einstein,\n",
      "Nearest to other: coke, pinus, canal, akshara, abet, nunnery, impotent, virtualization,\n",
      "Nearest to will: can, but, psych, to, utrecht, agouti, trinomial, typed,\n",
      "Nearest to one: two, four, three, circ, agouti, eight, six, operatorname,\n",
      "Nearest to into: from, within, with, in, by, norse, under, mugwort,\n",
      "Nearest to which: that, this, it, ablative, also, circ, acth, these,\n",
      "Nearest to of: in, and, for, nine, eight, agouti, with, six,\n",
      "Nearest to all: age, trails, coke, aberdeen, each, novella, alicante, boycott,\n",
      "Nearest to been: animated, was, by, are, were, climbers, gilchrist, concern,\n",
      "Nearest to five: eight, six, three, four, zero, seven, nine, two,\n",
      "Nearest to however: circ, ita, breakup, government, hun, exposing, that, folds,\n",
      "Nearest to s: and, zero, the, his, dasyprocta, circ, four, reginae,\n",
      "Nearest to d: b, and, cornerstone, criminal, vaux, falling, nazism, lille,\n",
      "Nearest to to: with, for, in, circ, nine, can, and, will,\n",
      "Nearest to are: were, is, have, was, be, agouti, circ, colleagues,\n",
      "Nearest to these: they, which, alumnus, bicycle, this, cultivating, the, consultative,\n",
      "Average loss at step  32000 :  5.98155181211\n",
      "Average loss at step  34000 :  5.75002421784\n",
      "Average loss at step  36000 :  5.7232785418\n",
      "Average loss at step  38000 :  5.49610832655\n",
      "Average loss at step  40000 :  5.2497041285\n",
      "Nearest to th: eight, six, agouti, zero, four, one, seven, solf,\n",
      "Nearest to other: coke, abet, pinus, canal, akshara, canine, agouti, nunnery,\n",
      "Nearest to will: can, to, but, psych, would, could, may, agouti,\n",
      "Nearest to one: two, four, three, six, eight, five, seven, zero,\n",
      "Nearest to into: from, within, under, with, zero, by, around, in,\n",
      "Nearest to which: that, this, it, also, ablative, but, circ, zero,\n",
      "Nearest to of: zero, in, and, agouti, victoriae, nine, for, operatorname,\n",
      "Nearest to all: age, trails, each, aberdeen, coke, novella, alicante, centrally,\n",
      "Nearest to been: animated, were, was, be, had, by, are, climbers,\n",
      "Nearest to five: four, three, six, eight, seven, zero, nine, two,\n",
      "Nearest to however: circ, government, ita, breakup, hun, that, which, four,\n",
      "Nearest to s: zero, and, the, lifecycle, dasyprocta, circ, his, recitative,\n",
      "Nearest to d: b, cornerstone, and, criminal, sulaiman, vaux, desktop, UNK,\n",
      "Nearest to to: zero, will, albury, nine, agouti, for, circ, with,\n",
      "Nearest to are: were, is, have, zero, was, be, circ, agouti,\n",
      "Nearest to these: they, which, some, alumnus, cultivating, their, calm, this,\n",
      "Average loss at step  42000 :  5.37650899339\n",
      "Average loss at step  44000 :  5.25256876504\n",
      "Average loss at step  46000 :  5.21673102832\n",
      "Average loss at step  48000 :  5.21116029644\n",
      "Average loss at step  50000 :  4.96895793116\n",
      "Nearest to th: six, one, eight, four, agouti, zero, solf, triple,\n",
      "Nearest to other: coke, abet, various, pinus, nunnery, canal, akshara, vortigern,\n",
      "Nearest to will: can, would, to, could, but, may, must, psych,\n",
      "Nearest to one: two, five, three, eight, six, four, seven, agouti,\n",
      "Nearest to into: from, within, in, with, under, around, bonus, and,\n",
      "Nearest to which: that, this, it, also, and, but, circ, ablative,\n",
      "Nearest to of: in, for, kapoor, eight, and, agouti, chanter, seven,\n",
      "Nearest to all: trails, age, each, aberdeen, coke, centrally, two, novella,\n",
      "Nearest to been: be, was, animated, were, climbers, had, are, gilchrist,\n",
      "Nearest to five: four, six, three, eight, seven, two, zero, agouti,\n",
      "Nearest to however: circ, kapoor, and, but, four, which, bhfiann, breakup,\n",
      "Nearest to s: zero, and, of, the, his, fripp, statistician, wagner,\n",
      "Nearest to d: b, cornerstone, criminal, and, lille, vaux, sulaiman, desktop,\n",
      "Nearest to to: albury, nine, will, could, for, and, would, agouti,\n",
      "Nearest to are: were, is, have, be, was, circ, agouti, collapsed,\n",
      "Nearest to these: they, some, which, many, kapoor, their, such, all,\n",
      "Average loss at step  52000 :  5.05772364521\n",
      "Average loss at step  54000 :  5.19200200772\n",
      "Average loss at step  56000 :  5.03186290598\n",
      "Average loss at step  58000 :  5.04041935813\n",
      "Average loss at step  60000 :  4.94470958161\n",
      "Nearest to th: six, four, eight, agouti, zero, one, five, michelob,\n",
      "Nearest to other: various, coke, abet, many, pinus, nunnery, these, three,\n",
      "Nearest to will: can, would, may, could, to, must, but, psych,\n",
      "Nearest to one: two, four, three, six, five, eight, wct, agouti,\n",
      "Nearest to into: from, within, under, around, novel, with, through, bonus,\n",
      "Nearest to which: that, this, but, also, it, circ, one, ablative,\n",
      "Nearest to of: in, nine, ursus, agouti, michelob, and, chanter, for,\n",
      "Nearest to all: trails, these, each, age, aberdeen, two, coke, some,\n",
      "Nearest to been: was, be, animated, were, climbers, had, by, become,\n",
      "Nearest to five: four, six, eight, three, seven, zero, nine, wct,\n",
      "Nearest to however: but, circ, kapoor, that, which, bhfiann, breakup, five,\n",
      "Nearest to s: zero, and, wct, his, fripp, dasyprocta, lifecycle, wagner,\n",
      "Nearest to d: b, cornerstone, criminal, UNK, sulaiman, lille, vaux, desktop,\n",
      "Nearest to to: will, albury, could, would, nine, can, agouti, for,\n",
      "Nearest to are: were, is, have, be, was, continuous, circ, collapsed,\n",
      "Nearest to these: some, many, they, such, all, kapoor, which, their,\n",
      "Average loss at step  62000 :  5.02076303279\n",
      "Average loss at step  64000 :  4.84122377086\n",
      "Average loss at step  66000 :  4.6281847496\n",
      "Average loss at step  68000 :  4.97596265435\n",
      "Average loss at step  70000 :  4.9011167326\n",
      "Nearest to th: six, eight, four, agouti, one, five, michelob, solf,\n",
      "Nearest to other: many, various, coke, abet, these, some, pinus, nunnery,\n",
      "Nearest to will: can, would, may, could, must, cebus, to, but,\n",
      "Nearest to one: four, two, six, five, three, eight, seven, dasyprocta,\n",
      "Nearest to into: from, within, under, through, around, novel, bonus, by,\n",
      "Nearest to which: that, this, it, but, also, circ, what, ablative,\n",
      "Nearest to of: chanter, and, michelob, agouti, ursus, for, kapoor, wct,\n",
      "Nearest to all: these, trails, some, each, aberdeen, many, coke, centrally,\n",
      "Nearest to been: be, was, animated, were, had, climbers, become, by,\n",
      "Nearest to five: four, six, eight, three, seven, nine, two, zero,\n",
      "Nearest to however: but, circ, kapoor, that, which, bhfiann, breakup, microcebus,\n",
      "Nearest to s: lifecycle, microcebus, his, and, wagner, fripp, statistician, zero,\n",
      "Nearest to d: b, cornerstone, UNK, criminal, sulaiman, desktop, wct, reginae,\n",
      "Nearest to to: will, albury, would, nine, could, can, not, cebus,\n",
      "Nearest to are: were, is, have, be, was, cebus, but, wct,\n",
      "Nearest to these: some, many, they, such, all, which, their, kapoor,\n",
      "Average loss at step  72000 :  4.75612296641\n",
      "Average loss at step  74000 :  4.80232468474\n",
      "Average loss at step  76000 :  4.72103777874\n",
      "Average loss at step  78000 :  4.80087538022\n",
      "Average loss at step  80000 :  4.79795696682\n",
      "Nearest to th: six, eight, five, agouti, four, seven, michelob, dartmoor,\n",
      "Nearest to other: many, various, coke, abet, some, these, officials, neighbourhoods,\n",
      "Nearest to will: can, would, may, could, must, to, might, cebus,\n",
      "Nearest to one: two, six, seven, three, four, eight, five, dasyprocta,\n",
      "Nearest to into: from, within, under, through, with, novel, around, by,\n",
      "Nearest to which: that, this, it, but, also, what, circ, busan,\n",
      "Nearest to of: michelob, in, wct, victoriae, agouti, ursus, chanter, kapoor,\n",
      "Nearest to all: these, some, each, trails, many, aberdeen, both, two,\n",
      "Nearest to been: be, was, become, were, had, climbers, animated, concern,\n",
      "Nearest to five: six, four, seven, eight, three, zero, nine, two,\n",
      "Nearest to however: but, circ, kapoor, that, busan, bhfiann, microcebus, wct,\n",
      "Nearest to s: zero, microcebus, lifecycle, wagner, his, bl, wct, dasyprocta,\n",
      "Nearest to d: b, cornerstone, seven, UNK, l, wct, reginae, criminal,\n",
      "Nearest to to: will, albury, would, could, agouti, circ, vma, can,\n",
      "Nearest to are: were, is, have, be, cebus, agouti, although, continuous,\n",
      "Nearest to these: many, some, such, they, all, which, their, other,\n",
      "Average loss at step  82000 :  4.76815943193\n",
      "Average loss at step  84000 :  4.75234839022\n",
      "Average loss at step  86000 :  4.76180783308\n",
      "Average loss at step  88000 :  4.74845406401\n",
      "Average loss at step  90000 :  4.75574306726\n",
      "Nearest to th: six, eight, agouti, five, seven, michelob, zero, linebarger,\n",
      "Nearest to other: various, many, some, abet, these, officials, coke, agouti,\n",
      "Nearest to will: can, would, may, could, must, might, to, cebus,\n",
      "Nearest to one: two, seven, four, three, five, eight, six, kapoor,\n",
      "Nearest to into: from, within, through, under, jati, with, novel, around,\n",
      "Nearest to which: that, this, but, also, circ, it, what, however,\n",
      "Nearest to of: chanter, michelob, in, ursus, victoriae, wct, kapoor, nine,\n",
      "Nearest to all: some, these, both, trails, many, each, aberdeen, kapoor,\n",
      "Nearest to been: be, was, become, were, climbers, animated, had, not,\n",
      "Nearest to five: seven, four, six, eight, three, zero, nine, two,\n",
      "Nearest to however: but, circ, kapoor, that, which, busan, bhfiann, wct,\n",
      "Nearest to s: microcebus, and, zero, wct, was, agouti, his, calypso,\n",
      "Nearest to d: b, cornerstone, UNK, l, reginae, wct, saito, miquelon,\n",
      "Nearest to to: will, would, jati, albury, can, vma, thaler, could,\n",
      "Nearest to are: were, have, is, be, cebus, although, include, if,\n",
      "Nearest to these: some, many, they, such, all, which, their, kapoor,\n",
      "Average loss at step  92000 :  4.66890357375\n",
      "Average loss at step  94000 :  4.73564189863\n",
      "Average loss at step  96000 :  4.68071707296\n",
      "Average loss at step  98000 :  4.5839034512\n",
      "Average loss at step  100000 :  4.68268930697\n",
      "Nearest to th: six, agouti, eight, five, seven, linebarger, michelob, nine,\n",
      "Nearest to other: various, many, some, abet, officials, these, nnn, including,\n",
      "Nearest to will: can, would, may, could, must, might, should, cebus,\n",
      "Nearest to one: six, seven, two, five, four, eight, three, dasyprocta,\n",
      "Nearest to into: from, within, through, under, jati, novel, around, with,\n",
      "Nearest to which: that, this, but, what, also, circ, and, however,\n",
      "Nearest to of: michelob, in, escuela, ursus, and, victoriae, including, agouti,\n",
      "Nearest to all: some, these, many, both, trails, each, two, aberdeen,\n",
      "Nearest to been: be, was, become, had, climbers, were, animated, by,\n",
      "Nearest to five: seven, four, six, three, eight, nine, two, zero,\n",
      "Nearest to however: but, circ, that, kapoor, which, busan, bhfiann, if,\n",
      "Nearest to s: microcebus, and, wct, agouti, bl, dasyprocta, calypso, recitative,\n",
      "Nearest to d: b, cornerstone, six, reginae, wct, UNK, l, agouti,\n",
      "Nearest to to: jati, will, vma, would, albury, agouti, could, peacocks,\n",
      "Nearest to are: were, is, have, although, be, include, collapsed, cebus,\n",
      "Nearest to these: many, some, such, all, they, their, which, calm,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = \"Nearest to %s:\" % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in range(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Please install sklearn, matplotlib, and scipy to visualize embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
